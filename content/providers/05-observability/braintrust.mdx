---
title: Braintrust
description: Monitoring and tracing LLM applications with Braintrust
---

# Braintrust Observability

Braintrust is an end-to-end platform for building AI applications. When building with the AI SDK, you can integrate Braintrust to [log](https://www.braintrust.dev/docs/guides/logging), monitor, and take action on real-world interactions.

## Setup

Braintrust offers two approaches for integrating with the AI SDK:

1. **OpenTelemetry (OTel)**: Standards-based tracing that works with your existing observability infrastructure
2. **Native Braintrust Wrapper**: Direct integration providing granular control and seamless access to Braintrust features

Choose the approach that best fits your project's needs and existing infrastructure.

### OpenTelemetry

Braintrust supports [AI SDK telemetry data](/docs/ai-sdk-core/telemetry).
To set up Braintrust as an [OpenTelemetry](https://opentelemetry.io/docs/) backend, you'll need to route the traces to Braintrust's OpenTelemetry endpoint, set your API key, and specify a parent project or experiment.

Once you set up an [OpenTelemetry Protocol Exporter](https://opentelemetry.io/docs/languages/js/exporters/) (OTLP) to send traces to Braintrust, we automatically convert LLM calls into Braintrust `LLM` spans, which can be saved as [prompts](https://www.braintrust.dev/docs/guides/functions/prompts) and evaluated in the [playground](https://www.braintrust.dev/docs/guides/playground).

To use the AI SDK to send telemetry data to Braintrust, set these environment variables in your Next.js app's `.env` file:

```bash
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.braintrust.dev/otel
OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer <Your API Key>, x-bt-parent=project_id:<Your Project ID>"
```

You can then use the `experimental_telemetry` option to enable telemetry on supported AI SDK function calls:

```typescript
import { createOpenAI } from '@ai-sdk/openai';
import { generateText } from 'ai';

const openai = createOpenAI();

async function main() {
  const result = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: 'What is 2 + 2?',
    experimental_telemetry: {
      isEnabled: true,
      metadata: {
        query: 'weather',
        location: 'San Francisco',
      },
    },
  });
  console.log(result);
}

main();
```

Traced LLM calls will appear under the Braintrust project or experiment provided in the `x-bt-parent` header.

### Native Braintrust Wrapper

As an alternative to OpenTelemetry, you can use Braintrust's native wrapper to automatically log your AI SDK requests. This approach provides more granular control over tracing and seamless integration with Braintrust's features.

```typescript
import { initLogger, wrapAISDKModel } from 'braintrust';
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

// Initialize the Braintrust logger
initLogger({
  projectName: 'My Project',
  apiKey: process.env.BRAINTRUST_API_KEY,
});

// Wrap the AI SDK model
const model = wrapAISDKModel(openai('gpt-4o-mini'));

async function main() {
  // This will automatically log the request, response, and metrics to Braintrust
  const result = await generateText({
    model: model,
    prompt: 'What is the capital of France?',
  });
  
  console.log(result.text);
}

main();
```

#### Wrapping Tools

You can also wrap tool implementations to log their executions:

```typescript
import { wrapTraced } from 'braintrust';
import { tool } from 'ai';
import { z } from 'zod';

const weatherTool = wrapTraced(
  tool({
    description: 'Get the current weather for a location',
    parameters: z.object({
      location: z.string().describe('The location to get weather for'),
    }),
    execute: async ({ location }) => {
      // Your tool implementation
      return {
        temperature: 72,
        condition: 'sunny',
        location,
      };
    },
  }),
  {
    name: 'weather_tool',
  }
);
```

## Choosing Between OpenTelemetry and Native Wrapper

- **Use OpenTelemetry** if you:
  - Have existing OpenTelemetry infrastructure
  - Want standards-based tracing
  - Need to send traces to multiple backends
  - Prefer minimal code changes

- **Use Native Wrapper** if you:
  - Want granular control over tracing
  - Need tight integration with Braintrust features
  - Want to wrap specific models or tools
  - Prefer explicit logging control

## Resources

To see a step-by-step example, check out the Braintrust [cookbook](https://www.braintrust.dev/docs/cookbook/recipes/OTEL-logging).

After you log your application in Braintrust, explore other workflows like:

- Adding [tools](https://www.braintrust.dev/docs/guides/functions/tools) to your library and using them in [experiments](https://www.braintrust.dev/docs/guides/evals) and the [playground](https://www.braintrust.dev/docs/guides/playground)
- Creating [custom scorers](https://www.braintrust.dev/docs/guides/functions/scorers) to assess the quality of your LLM calls
- Adding your logs to a [dataset](https://www.braintrust.dev/docs/guides/datasets) and running evaluations comparing models and prompts
